{"parents": [{"link": "../../../../", "title": "Module code"}], "title": "steamship.agents.llms.openai", "body": "<h1>Source code for steamship.agents.llms.openai</h1><div class=\"highlight\"><pre>\n<span></span><span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Optional</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">steamship</span> <span class=\"kn\">import</span> <span class=\"n\">Block</span><span class=\"p\">,</span> <span class=\"n\">File</span><span class=\"p\">,</span> <span class=\"n\">PluginInstance</span><span class=\"p\">,</span> <span class=\"n\">Steamship</span>\n<span class=\"kn\">from</span> <span class=\"nn\">steamship.agents.schema</span> <span class=\"kn\">import</span> <span class=\"n\">LLM</span><span class=\"p\">,</span> <span class=\"n\">ChatLLM</span><span class=\"p\">,</span> <span class=\"n\">Tool</span>\n\n<span class=\"n\">PLUGIN_HANDLE</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;gpt-4&quot;</span>\n<span class=\"n\">DEFAULT_MAX_TOKENS</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>\n\n\n<div class=\"viewcode-block\" id=\"OpenAI\"><a class=\"viewcode-back\" href=\"../../../../../api/steamship.agents.llms/#steamship.agents.llms.openai.OpenAI\">[docs]</a><span class=\"k\">class</span> <span class=\"nc\">OpenAI</span><span class=\"p\">(</span><span class=\"n\">LLM</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;LLM that uses Steamship&#39;s OpenAI plugin to generate completions.</span>\n\n<span class=\"sd\">    NOTE: By default, this LLM uses the `gpt-3.5-turbo` model. Valid model</span>\n<span class=\"sd\">    choices are `gpt-3.5-turbo` and `gpt-4`.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"n\">generator</span><span class=\"p\">:</span> <span class=\"n\">PluginInstance</span>\n    <span class=\"n\">client</span><span class=\"p\">:</span> <span class=\"n\">Steamship</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;gpt-3.5-turbo&quot;</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span>\n    <span class=\"p\">):</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Create a new instance.</span>\n\n<span class=\"sd\">        Valid model names are:</span>\n<span class=\"sd\">         - gpt-4</span>\n<span class=\"sd\">         - gpt-3.5-turbo</span>\n\n<span class=\"sd\">        Supported kwargs include:</span>\n<span class=\"sd\">        - `max_tokens` (controls the size of LLM responses)</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">client</span>\n        <span class=\"n\">max_tokens</span> <span class=\"o\">=</span> <span class=\"n\">DEFAULT_MAX_TOKENS</span>\n        <span class=\"k\">if</span> <span class=\"s2\">&quot;max_tokens&quot;</span> <span class=\"ow\">in</span> <span class=\"n\">kwargs</span><span class=\"p\">:</span>\n            <span class=\"n\">max_tokens</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s2\">&quot;max_tokens&quot;</span><span class=\"p\">]</span>\n\n        <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">use_plugin</span><span class=\"p\">(</span>\n            <span class=\"n\">PLUGIN_HANDLE</span><span class=\"p\">,</span>\n            <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;model&quot;</span><span class=\"p\">:</span> <span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"s2\">&quot;temperature&quot;</span><span class=\"p\">:</span> <span class=\"n\">temperature</span><span class=\"p\">,</span> <span class=\"s2\">&quot;max_tokens&quot;</span><span class=\"p\">:</span> <span class=\"n\">max_tokens</span><span class=\"p\">},</span>\n        <span class=\"p\">)</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"o\">=</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">generator</span><span class=\"o\">=</span><span class=\"n\">generator</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n<div class=\"viewcode-block\" id=\"OpenAI.complete\"><a class=\"viewcode-back\" href=\"../../../../../api/steamship.agents.llms/#steamship.agents.llms.openai.OpenAI.complete\">[docs]</a>    <span class=\"k\">def</span> <span class=\"nf\">complete</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">stop</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Block</span><span class=\"p\">]:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Completes the prompt, respecting the supplied stop sequence.</span>\n\n<span class=\"sd\">        Supported kwargs include:</span>\n<span class=\"sd\">        - `max_tokens` (controls the size of LLM responses)</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n        <span class=\"k\">if</span> <span class=\"n\">stop</span><span class=\"p\">:</span>\n            <span class=\"n\">options</span><span class=\"p\">[</span><span class=\"s2\">&quot;stop&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">stop</span>\n\n        <span class=\"k\">if</span> <span class=\"s2\">&quot;max_tokens&quot;</span> <span class=\"ow\">in</span> <span class=\"n\">kwargs</span><span class=\"p\">:</span>\n            <span class=\"n\">options</span><span class=\"p\">[</span><span class=\"s2\">&quot;max_tokens&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s2\">&quot;max_tokens&quot;</span><span class=\"p\">]</span>\n\n        <span class=\"n\">action_task</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">generator</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">prompt</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"n\">options</span><span class=\"p\">)</span>\n        <span class=\"n\">action_task</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"n\">action_task</span><span class=\"o\">.</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">blocks</span></div></div>\n\n\n<div class=\"viewcode-block\" id=\"ChatOpenAI\"><a class=\"viewcode-back\" href=\"../../../../../api/steamship.agents.llms/#steamship.agents.llms.openai.ChatOpenAI\">[docs]</a><span class=\"k\">class</span> <span class=\"nc\">ChatOpenAI</span><span class=\"p\">(</span><span class=\"n\">ChatLLM</span><span class=\"p\">,</span> <span class=\"n\">OpenAI</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;ChatLLM that uses Steamship&#39;s OpenAI plugin to generate chat completions.&quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;gpt-4-0613&quot;</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Create a new instance.</span>\n\n<span class=\"sd\">        Valid model names are:</span>\n<span class=\"sd\">         - gpt-4</span>\n<span class=\"sd\">         - gpt-4-0613</span>\n\n<span class=\"sd\">        Supported kwargs include:</span>\n<span class=\"sd\">        - `max_tokens` (controls the size of LLM responses)</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"o\">=</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n<div class=\"viewcode-block\" id=\"ChatOpenAI.chat\"><a class=\"viewcode-back\" href=\"../../../../../api/steamship.agents.llms/#steamship.agents.llms.openai.ChatOpenAI.chat\">[docs]</a>    <span class=\"k\">def</span> <span class=\"nf\">chat</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">messages</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Block</span><span class=\"p\">],</span> <span class=\"n\">tools</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Tool</span><span class=\"p\">]],</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Block</span><span class=\"p\">]:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Sends chat messages to the LLM with functions from the supplied tools in a side-channel.</span>\n\n<span class=\"sd\">        Supported kwargs include:</span>\n<span class=\"sd\">        - `max_tokens` (controls the size of LLM responses)</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n\n        <span class=\"n\">temp_file</span> <span class=\"o\">=</span> <span class=\"n\">File</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">blocks</span><span class=\"o\">=</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n\n        <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n        <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">functions</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n            <span class=\"k\">for</span> <span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"n\">tools</span><span class=\"p\">:</span>\n                <span class=\"n\">functions</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">tool</span><span class=\"o\">.</span><span class=\"n\">as_openai_function</span><span class=\"p\">())</span>\n            <span class=\"n\">options</span><span class=\"p\">[</span><span class=\"s2\">&quot;functions&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">functions</span>\n\n        <span class=\"k\">if</span> <span class=\"s2\">&quot;max_tokens&quot;</span> <span class=\"ow\">in</span> <span class=\"n\">kwargs</span><span class=\"p\">:</span>\n            <span class=\"n\">options</span><span class=\"p\">[</span><span class=\"s2\">&quot;max_tokens&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s2\">&quot;max_tokens&quot;</span><span class=\"p\">]</span>\n\n        <span class=\"n\">tool_selection_task</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">generator</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">input_file_id</span><span class=\"o\">=</span><span class=\"n\">temp_file</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"o\">=</span><span class=\"n\">options</span><span class=\"p\">)</span>\n        <span class=\"n\">tool_selection_task</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"n\">tool_selection_task</span><span class=\"o\">.</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">blocks</span></div></div>\n</pre></div>", "current_page_name": "_modules/steamship/agents/llms/openai", "sidebars": ["sidebar/brand.html", "sidebar/search.html", "sidebar/scroll-start.html", "sidebar/navigation.html", "sidebar/ethical-ads.html", "sidebar/scroll-end.html", "sidebar/variant-selector.html"], "customsidebar": null, "favicon_url": null, "logo_url": null, "alabaster_version": "0.7.12", "furo_version": "2023.03.27", "furo_navigation_tree": "", "furo_hide_toc": true, "furo_pygments": {"light": {"background": "#f8f8f8", "foreground": "black"}, "dark": {"background": "#272822", "foreground": "#f8f8f2"}}}