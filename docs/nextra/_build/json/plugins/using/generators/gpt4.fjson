{"parents": [{"link": "../../../", "title": "Plugins"}, {"link": "../../", "title": "Using Plugins"}, {"link": "../", "title": "Generators"}], "prev": {"link": "../dalle/", "title": "DALL-E"}, "next": {"link": "../../embedders/", "title": "Embedders"}, "title": "GPT-4 (and GPT 3.5)", "meta": null, "body": "<section id=\"gpt-4-and-gpt-3-5\">\n<span id=\"gpt4\"></span><h1>GPT-4 (and GPT 3.5)<a class=\"headerlink\" href=\"#gpt-4-and-gpt-3-5\" title=\"Permalink to this heading\">#</a></h1>\n<p>The GPT-4 <code class=\"docutils literal notranslate\"><span class=\"pre\">Generator</span></code> plugin uses OpenAI\u2019s GPT-4 to generate text from a text prompt,\nor the continuation of a chat. It can also be used with GPT-3.5 by passing <code class=\"docutils literal notranslate\"><span class=\"pre\">&quot;gpt-3.5-turbo&quot;</span></code>\nas the <code class=\"docutils literal notranslate\"><span class=\"pre\">model</span></code> configuration parameter.</p>\n<p>The plugin will treat each <code class=\"docutils literal notranslate\"><span class=\"pre\">Block</span></code> of the input as an element of a chat. If a <code class=\"docutils literal notranslate\"><span class=\"pre\">Block</span></code> has\na <code class=\"docutils literal notranslate\"><span class=\"pre\">Tag</span></code> of kind \u201crole\u201d and name ( \u201csystem\u201d | \u201cuser\u201d | \u201cassistant\u201d ), the content will be passed\nto OpenAI with the corresponding role. If a <code class=\"docutils literal notranslate\"><span class=\"pre\">Block</span></code> does not have a role tag, it will\nbe passed with the configured default role, which defaults to \u201cuser\u201d (see config params below).</p>\n<p>The plugin handles retrying for rate limits and uses Steamship\u2019s OpenAI API key by default,\neliminating the need for you to have a separate OpenAI account.</p>\n<p>The simplest possible example is:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">gpt4</span> <span class=\"o\">=</span> <span class=\"n\">steamship</span><span class=\"o\">.</span><span class=\"n\">use_plugin</span><span class=\"p\">(</span><span class=\"s2\">&quot;gpt-4&quot;</span><span class=\"p\">)</span>\n\n<span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"n\">gpt4</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"s2\">&quot;Tell me a joke&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">()</span>\n<span class=\"n\">joke</span> <span class=\"o\">=</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">blocks</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">text</span>\n</pre></div>\n</div>\n<p>To build a chat interaction, you can persist the prompt components to a <code class=\"docutils literal notranslate\"><span class=\"pre\">File</span></code> object,\ntagging them with their conversational roles:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">gpt4</span> <span class=\"o\">=</span> <span class=\"n\">steamship</span><span class=\"o\">.</span><span class=\"n\">use_plugin</span><span class=\"p\">(</span><span class=\"s2\">&quot;gpt-4&quot;</span><span class=\"p\">)</span>\n\n<span class=\"n\">chat_file</span> <span class=\"o\">=</span> <span class=\"n\">File</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">blocks</span><span class=\"o\">=</span><span class=\"p\">[</span>\n    <span class=\"n\">Block</span><span class=\"p\">(</span>\n        <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"s2\">&quot;You are an assistant who likes to tell jokes about bananas&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">tags</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">Tag</span><span class=\"p\">(</span><span class=\"n\">kind</span><span class=\"o\">=</span><span class=\"n\">TagKind</span><span class=\"o\">.</span><span class=\"n\">ROLE</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">RoleTag</span><span class=\"o\">.</span><span class=\"n\">SYSTEM</span><span class=\"p\">)]</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">chat_file</span><span class=\"o\">.</span><span class=\"n\">append_block</span><span class=\"p\">(</span>\n    <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"s2\">&quot;Do you know any fruit jokes?&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">tags</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">Tag</span><span class=\"p\">(</span><span class=\"n\">kind</span><span class=\"o\">=</span><span class=\"n\">TagKind</span><span class=\"o\">.</span><span class=\"n\">ROLE</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">RoleTag</span><span class=\"o\">.</span><span class=\"n\">USER</span><span class=\"p\">)]</span>\n<span class=\"p\">)</span>\n<span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"n\">gpt4</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span>\n    <span class=\"n\">input_file_id</span><span class=\"o\">=</span><span class=\"n\">chat_file</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">,</span>\n    <span class=\"n\">append_output_to_file</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"n\">output_file_id</span><span class=\"o\">=</span><span class=\"n\">chat_file</span><span class=\"o\">.</span><span class=\"n\">id</span>\n<span class=\"p\">)</span>\n<span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">()</span>\n<span class=\"n\">joke</span> <span class=\"o\">=</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">blocks</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">text</span>\n</pre></div>\n</div>\n<p>In the example above, in addition to being returned as the result of the <code class=\"docutils literal notranslate\"><span class=\"pre\">Task</span></code>, the output\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Block</span></code> is appended to <code class=\"docutils literal notranslate\"><span class=\"pre\">chat_file</span></code>.</p>\n<p>All output <code class=\"docutils literal notranslate\"><span class=\"pre\">Blocks</span></code> will be tagged with the \u201cassistant\u201d role to allow more\ncontent to be easily appended and generated.</p>\n<p>The <a class=\"reference internal\" href=\"../#generators\"><span class=\"std std-ref\">Generator</span></a> interface supports many other ways to provide input and persist output.</p>\n<p>The GPT-4 plugin has a few <a class=\"reference internal\" href=\"../../#creating-plugin-instances\"><span class=\"std std-ref\">configuration parameters</span></a>:</p>\n<ul class=\"simple\">\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">openai_api_key</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">str</span></code>, An openAI API key to use. If left default, will use Steamship\u2019s API key.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">max_tokens</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code>, default 256, The maximum number of tokens to generate per request. Can be overridden in runtime options.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">model</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">str</span></code> , default \u201cgpt-4\u201d, The OpenAI model to use. Can be a pre-existing fine-tuned model.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">temperature</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">float</span></code> , default 0.4, Controls randomness. Lower values produce higher likelihood / more predictable results; higher values produce more variety. Values between 0-1.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">top_p</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code>, default 1, Controls the nucleus sampling, where the model considers the results of the tokens with top_p probability mass. Values between 0-1.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">presence_penalty</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code>, default 0, Control how likely the model will reuse words. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\u2019s likelihood to talk about new topics. Number between -2.0 and 2.0.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">frequency_penalty</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code>, default 0, Control how likely the model will reuse words. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\u2019s likelihood to repeat the same line verbatim. Number between -2.0 and 2.0.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">moderate_output</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">bool</span></code> , default True, Pass the generated output back through OpenAI\u2019s moderation endpoint and throw an exception if flagged.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">max_retries</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code> , default 8, Maximum number of retries to make when generating.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">request_timeout</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">float</span></code>, default 600, Timeout for requests to OpenAI completion API. Default is 600 seconds.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">n</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code>, default 1, How many completions to generate for each prompt.</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">default_role</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">str</span></code>, default RoleTag.USER, The default role to use for a block that does not have a Tag of kind=\u2019role\u2019</p></li>\n<li><p><code class=\"docutils literal notranslate\"><span class=\"pre\">default_system_prompt</span></code>: <code class=\"docutils literal notranslate\"><span class=\"pre\">str</span></code> , default \u201c\u201d, System prompt that will be prepended before every request</p></li>\n</ul>\n<p>Additionally, stopwords can be passed in the <code class=\"docutils literal notranslate\"><span class=\"pre\">stop</span></code> parameter in the <code class=\"docutils literal notranslate\"><span class=\"pre\">options</span></code> of the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">generate</span></code> call. Other parameters may be overridden on an individual invocation by passing\nthem in the <code class=\"docutils literal notranslate\"><span class=\"pre\">options</span></code> as well.</p>\n</section>\n", "metatags": "<meta name=\"generator\" content=\"Docutils 0.17.1: http://docutils.sourceforge.net/\" />\n", "rellinks": [["genindex", "General Index", "I", "index"], ["py-modindex", "Python Module Index", "", "modules"], ["plugins/using/embedders/index", "Embedders", "N", "next"], ["plugins/using/generators/dalle", "DALL-E", "P", "previous"]], "sourcename": "plugins/using/generators/gpt4.rst.txt", "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">GPT-4 (and GPT 3.5)</a></li>\n</ul>\n", "display_toc": false, "page_source_suffix": ".rst", "current_page_name": "plugins/using/generators/gpt4", "sidebars": ["sidebar/brand.html", "sidebar/search.html", "sidebar/scroll-start.html", "sidebar/navigation.html", "sidebar/ethical-ads.html", "sidebar/scroll-end.html", "sidebar/variant-selector.html"], "customsidebar": null, "favicon_url": null, "logo_url": null, "alabaster_version": "0.7.12", "furo_version": "2023.03.27", "furo_navigation_tree": "", "furo_hide_toc": true, "furo_pygments": {"light": {"background": "#f8f8f8", "foreground": "black"}, "dark": {"background": "#272822", "foreground": "#f8f8f2"}}}